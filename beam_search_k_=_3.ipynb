{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "beam_search_k_=_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2101bd95b8134007b36e552ec07655fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8df99574ee4e44e0979e106345ff1e3f",
              "IPY_MODEL_975a6c2809c74ab2b0d432c012b18826"
            ],
            "layout": "IPY_MODEL_450e92ad344140d595ec723464737d17"
          }
        },
        "450e92ad344140d595ec723464737d17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8df99574ee4e44e0979e106345ff1e3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_731115d2d11048779c2f3d4ace560a92",
            "max": 1164,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_395b8bbcb185443794d5bda9d08a3ea3",
            "value": 1164
          }
        },
        "975a6c2809c74ab2b0d432c012b18826": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3495e737d524446fb6e01139bc88e718",
            "placeholder": "​",
            "style": "IPY_MODEL_daf07a1846204eaa88e2daee9a295ab6",
            "value": " 1164/1164 [00:45&lt;00:00, 25.58it/s]"
          }
        },
        "395b8bbcb185443794d5bda9d08a3ea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "731115d2d11048779c2f3d4ace560a92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "daf07a1846204eaa88e2daee9a295ab6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3495e737d524446fb6e01139bc88e718": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "axwBS8jGworv",
        "outputId": "6b02093d-8825-4565-e172-91b648254a1f"
      },
      "source": [
        "!pip install sacrebleu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.6/dist-packages (1.4.9)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (3.6.6)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (1.7.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8bSPROeEiV9"
      },
      "source": [
        "import re\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import sacrebleu\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "# import nltk\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7hyvB_QSFrWb",
        "outputId": "9a0f58d0-0627-4e08-b3ae-8c57fdc534cd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCk4XGwaN8sM"
      },
      "source": [
        "train_file = '/content/eng-fra.txt'\n",
        "test_file = '/content/testeng-fra.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBGi-PLsEZNo"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu2LjBl5EZN-"
      },
      "source": [
        "Class Lang is to create word2index and index2word mapping dictionaries for both languages. The first 3 index-word pairs are ``{0: \"SOS\", 1: \"EOS\", 2: \"PAD\"}``"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hiv5_KFKEZN_"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "PAD_token = 2\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"PAD\"}\n",
        "        self.n_words = 3  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wbb5KhRYEZOI"
      },
      "source": [
        "These functions are used to cleanse the data - remove punctuations, remove numbers and take only ASCII characters.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxxQXzUvEZOK"
      },
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNuZhGy0EZOa"
      },
      "source": [
        "To read the data file we will split the file into lines, and then split\n",
        "lines into pairs. The files are all English → Other Language, so if we\n",
        "want to translate from Other Language → English I added the ``reverse``\n",
        "flag to reverse the pairs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV1OWrWLEZOc"
      },
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('/content/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "    print(input_lang, output_lang, pairs)\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtR5JoVWEZOu"
      },
      "source": [
        "For this particular dataset, most of the sentences are of very big lengths. Hence, we fix a big value for the MAX_LENGTH. The functions below filters out sentences which have number of words less than MAX_LENGTH\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9K9dYcMEZOx"
      },
      "source": [
        "MAX_LENGTH = 20\n",
        "\n",
        "# eng_prefixes = (\n",
        "#     \"i am \", \"i m \",\n",
        "#     \"he is\", \"he s \",\n",
        "#     \"she is\", \"she s\",\n",
        "#     \"you are\", \"you re \",\n",
        "#     \"we are\", \"we re \",\n",
        "#     \"they are\", \"they re \"\n",
        "# )\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH #and \\\n",
        "        # p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63YqL8zrEZO-"
      },
      "source": [
        "The full process for preparing the data is:\n",
        "\n",
        "-  Read text file and split into lines, split lines into pairs\n",
        "-  Normalize text, filter by length and content\n",
        "-  Make word lists from sentences in pairs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPzxZLMuEZPA"
      },
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAhfLdpOEZPQ"
      },
      "source": [
        "The Seq2Seq Model\n",
        "=================\n",
        "\n",
        "We have implemented the seq2seq model below using GRUs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHK_ikycEZPS"
      },
      "source": [
        "The Encoder\n",
        "-----------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yde300mSqYA3"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "hidden_size = 512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grGyXXD4EZPT"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        # self.embedding.weight = torch.nn.Parameter(init_embeddings)\n",
        "        \n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, -1, self.hidden_size)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self, batch_size):\n",
        "        result = Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))\n",
        "        if use_cuda:\n",
        "            return result.cuda()\n",
        "        else:\n",
        "            return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoLEDhlQEZP2"
      },
      "source": [
        "Attention Decoder\n",
        "-----------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cfc9khtGEZP4"
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        # self.embedding.weight = torch.nn.Parameter(init_embeddings)\n",
        "        \n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, self.n_layers)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, -1, self.hidden_size)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)))\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).transpose(0, 1)\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]))\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self, batch_size):\n",
        "        result = Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))\n",
        "        if use_cuda:\n",
        "            return result.cuda()\n",
        "        else:\n",
        "            return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RUnzOBbEZQB"
      },
      "source": [
        "\n",
        "\n",
        "Training\n",
        "========\n",
        "\n",
        "Preparing Training Data\n",
        "-----------------------\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udC4EKz5EZQC"
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    indexes = indexes + [PAD_token] * (MAX_LENGTH - len(indexes)) \n",
        "    # print('indexes', indexes)\n",
        "    result = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
        "    if use_cuda:\n",
        "        return result.cuda()\n",
        "    else:\n",
        "        return result\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_variable = tensorFromSentence(input_lang, pair[0])\n",
        "    target_variable = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_variable, target_variable)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJLDTukIEZQM"
      },
      "source": [
        "Training the Model\n",
        "------------------\n",
        "Here, we are not using teacher forsing. The variable ``use_teacher_forcing`` has been set to ``False``\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFJDnKlZEZQO"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden(BATCH_SIZE)\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = MAX_LENGTH\n",
        "    target_length = MAX_LENGTH\n",
        "    \n",
        "    encoder_outputs = Variable(torch.zeros(BATCH_SIZE, max_length, encoder.hidden_size))\n",
        "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
        "    \n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_variable[ei], encoder_hidden)\n",
        "        encoder_outputs[:, ei] = encoder_output[0]\n",
        "\n",
        "    decoder_input = Variable(torch.LongTensor([[SOS_token]] * BATCH_SIZE))\n",
        "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
        "    \n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = False #if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_variable[di])\n",
        "            decoder_input = target_variable[di]  # Teacher forcing\n",
        "\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.data.topk(k=1, dim=1)\n",
        "            \n",
        "            decoder_input = Variable(torch.LongTensor([[ni] for ni in topi[:, 0]]))\n",
        "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
        "\n",
        "            loss += criterion(decoder_output, target_variable[di])\n",
        "\n",
        "    loss.backward()    \n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhJKAkWNEZQZ"
      },
      "source": [
        "This is a helper function to print time elapsed and estimated time\n",
        "remaining given the current time and progress %.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY8a15T4EZQc"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inS8D9ILEZQp"
      },
      "source": [
        "The whole training process looks like this:\n",
        "\n",
        "-  Start a timer\n",
        "-  Initialize optimizers and criterion\n",
        "-  Create set of training pairs\n",
        "-  Start empty losses array for plotting\n",
        "\n",
        "Then we call ``train`` many times and occasionally print the progress (%\n",
        "of examples, time so far, estimated time) and average loss.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iePTVufaEZQs"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(BATCH_SIZE)]\n",
        "        input_variable = torch.cat([pair[0] for pair in training_pairs], 1)\n",
        "        target_variable = torch.cat([pair[1] for pair in training_pairs], 1)\n",
        "        loss = train(input_variable, target_variable, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "            \n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "            # evaluateRandomly(encoder, decoder)\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk6p1OXoEZRA"
      },
      "source": [
        "Plotting results\n",
        "----------------\n",
        "\n",
        "Plotting is done with matplotlib, using the array of loss values\n",
        "``plot_losses`` saved while training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD-2rKy6EZRC"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcAlzBrWEZRS"
      },
      "source": [
        "Evaluation\n",
        "==========\n",
        "\n",
        "Evaluation is mostly the same as training, but there are no targets so\n",
        "we simply feed the decoder's predictions back to itself for each step.\n",
        "Every time it predicts a word we add it to the output string, and if it\n",
        "predicts the EOS token we stop there. We also store the decoder's\n",
        "attention outputs for display later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II4jWkexEZRV"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH, beam_width=3):\n",
        "    input_variable = tensorFromSentence(input_lang, sentence)\n",
        "    input_length = input_variable.size()[0]\n",
        "    encoder_hidden = encoder.initHidden(beam_width)\n",
        "\n",
        "    input_variable = torch.cat([input_variable for i in range(beam_width)], 1)\n",
        "    encoder_outputs = Variable(torch.zeros(beam_width, max_length, encoder.hidden_size))\n",
        "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_variable[ei],\n",
        "                                                 encoder_hidden)\n",
        "        encoder_outputs[:, ei] = encoder_output[0]\n",
        "\n",
        "    decoder_input = Variable(torch.LongTensor([[SOS_token]] * beam_width))  # SOS\n",
        "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_hidden_copy = Variable(torch.zeros(encoder1.n_layers, beam_width, hidden_size))\n",
        "    decoder_hidden_copy = decoder_hidden_copy.cuda() if use_cuda else decoder_hidden_copy\n",
        "    \n",
        "    decoded_words = []\n",
        "    decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "    probability = torch.zeros(beam_width).view(-1, 1)\n",
        "    probability = probability.cuda() if use_cuda else probability\n",
        "    \n",
        "    prev = [[]] * max_length\n",
        "    idxs = [[]] * max_length\n",
        "    \n",
        "    cands = []\n",
        "    values = []\n",
        "    for di in range(max_length):\n",
        "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "            decoder_input, decoder_hidden, encoder_outputs)\n",
        "          \n",
        "#       LogSoftmax\n",
        "        topv, topi = decoder_output.data.topk(beam_width)\n",
        "        if di == 0:\n",
        "            decoder_input = Variable(torch.LongTensor([[ni] for ni in topi[0]]))\n",
        "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
        "            probability = topv[0].view(-1, 1)\n",
        "        else:\n",
        "            # print('printing topi', topi)\n",
        "#           Get beam_width candidate for each beam.\n",
        "            topv = topv + probability\n",
        "            topv, topi = topv.view(-1), topi.view(-1)\n",
        "\n",
        "#           Select beam_width from beam_width*beam_width.\n",
        "            _, topt = topv.topk(beam_width)\n",
        "\n",
        "#           Update adn prepare for the next step.\n",
        "            probability = topv[topt]\n",
        "            decoder_input = Variable(torch.LongTensor([[topi[topt[k]]] for k in range(beam_width)]))\n",
        "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
        "            prev[di] = [topt[k] // beam_width for k in range(beam_width)]\n",
        "            \n",
        "#           Don't forget to prepare the corresponding hidden state.\n",
        "            for i in range(beam_width):\n",
        "                decoder_hidden_copy[:, i] = decoder_hidden[:, prev[di][i]]\n",
        "            decoder_hidden = decoder_hidden_copy.clone()\n",
        "            \n",
        "        for i in range(beam_width):\n",
        "            values.append([decoder_input.data[i, 0].item(), di, i])\n",
        "            if decoder_input.data[i, 0] == PAD_token and probability[i] > -np.inf:\n",
        "                cands.append([probability[i].item(), di, i])\n",
        "                probability[i] = -np.inf\n",
        "    max_proba = -500000000000000\n",
        "    k = 0\n",
        "    word_len = 0\n",
        "    for i in range(len(cands)):\n",
        "        proba = cands[i][0] \n",
        "        if(proba>max_proba):\n",
        "            max_proba = proba\n",
        "            k = cands[i][2] \n",
        "            word_len = cands[i][1]+1\n",
        "    final_list = [output_lang.index2word[a] for a,b,c in values if c==k][:word_len]\n",
        "    return final_list, decoder_attentions[:di + 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcLl2gejEZRl"
      },
      "source": [
        "We can evaluate random sentences from the training set and print out the\n",
        "input, target, and output to make some subjective quality judgements:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXl99bPLEZRn"
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words).replace('PAD', '').replace('EOS', '')\n",
        "        print('<', output_sentence)\n",
        "        print('')\n",
        "        bleu = sacrebleu.corpus_bleu(pair[1], output_sentence)\n",
        "        print(\"Score=\",bleu.score)        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYaauUwwEZRy"
      },
      "source": [
        "Training and Evaluating\n",
        "=======================\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ry0V4GwEZR0"
      },
      "source": [
        "# hidden_size = 256\n",
        "# encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "# attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "# trainIters(encoder1, attn_decoder1, 75000, print_every=5000)\n",
        "\n",
        "# # attn_decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "# # trainIters(encoder1, attn_decoder1, 75000, print_every=5000)\n",
        "\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size, n_layers=2).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, n_layers=2, dropout_p=0.1).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec0MSXoUOm0h"
      },
      "source": [
        "trainIters(encoder1, attn_decoder1, 4000, print_every=50, plot_every=200, learning_rate=1e-3)\n",
        "torch.save(encoder1.state_dict(), 'encoder_beam.params')\n",
        "torch.save(attn_decoder1.state_dict(), 'decoder_beam.params')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ynH65OxOKWYJ",
        "outputId": "bb819b06-f9cb-47b3-9eb5-af264bb9afc5"
      },
      "source": [
        "encoder1.load_state_dict(torch.load('encoder_beam.params', map_location={'cuda:0': 'cpu'}))\n",
        "attn_decoder1.load_state_dict(torch.load('decoder_beam.params', map_location={'cuda:0': 'cpu'}))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 961
        },
        "id": "IMtvfIAMEZSA",
        "outputId": "b5686250-ae1b-4dac-9cc0-d85a1a1e0867"
      },
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> les gens devenaient fous .\n",
            "= people were going crazy .\n",
            "< people were going crazy .  \n",
            "\n",
            "Score= 100.00000000000004\n",
            "> sinon va embeter quelqu un d autre aujourd hui .\n",
            "= otherwise go bother somebody else today .\n",
            "< otherwise go bother somebody else today .  \n",
            "\n",
            "Score= 100.00000000000004\n",
            "> et pour nathaniel la musique c est la sante mentale .\n",
            "= and for nathaniel music is sanity .\n",
            "< and for nathaniel music is sanity .  \n",
            "\n",
            "Score= 100.00000000000004\n",
            "> jusqu a ce que finalement j aie au telephone le biologiste en chef .\n",
            "= until finally i got on the phone with the head biologist .\n",
            "< until finally i got on the phone with the head biologist .  \n",
            "\n",
            "Score= 100.00000000000004\n",
            "> objectivement nous faisons en general mieux et nous nous sentons moins bien .\n",
            "= and so the net result is that we do better in general objectively and we feel worse .\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "< and so the net result is that we do better in general objectively and we feel worse .  \n",
            "\n",
            "Score= 100.00000000000004\n",
            "> vous l avez peut etre lu nous avons insere des signatures .\n",
            "= you may have read that we put watermarks in .\n",
            "< you may have read that we put watermarks in .  \n",
            "\n",
            "Score= 100.00000000000004\n",
            "> il est impossible de de le percevoir correctement .\n",
            "= there s just no way of getting it right .\n",
            "< there s just no way of getting it right .  \n",
            "\n",
            "Score= 100.00000000000004\n",
            "> une productivite merveilleuse . j aime cette idee .\n",
            "= blissful productivity . i love it .\n",
            "< blissful productivity . i love it .  \n",
            "\n",
            "Score= 100.00000000000004\n",
            "> on en parle de facon ambivalente .\n",
            "= we talk about it ambivalently .\n",
            "< we talk about it ambivalently .  \n",
            "\n",
            "Score= 100.00000000000004\n",
            "> a cette epoque j aspirais a ce genre de normalite .\n",
            "= i aspired to that kind of normalcy back then .\n",
            "< i aspired to that kind of normalcy back then .  \n",
            "\n",
            "Score= 100.00000000000004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0r1-Q1LXy8K"
      },
      "source": [
        "Testing\n",
        "======="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p3WKAntX2ce"
      },
      "source": [
        "def read_test(test_file, reverse = False):\n",
        "    # Read the file and split into lines\n",
        "    lines = open(test_file, encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "    return pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE6rx6uKaHsR"
      },
      "source": [
        "def evaluateFully(encoder, decoder, test_file):\n",
        "    evaluated = []\n",
        "    pairs = read_test(test_file, reverse = True)\n",
        "    pairs = filterPairs(pairs)\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    pairs0 = [p[0] for p in pairs]\n",
        "    pairs1 = [p[1] for p in pairs]\n",
        "    start = time.time()\n",
        "    for i in tqdm(range(len(pairs0))):\n",
        "        pair0 = pairs0[i]\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair0)\n",
        "        output_words = [word for word in output_words if word not in ['PAD', 'EOS']]\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        evaluated.append(output_sentence)\n",
        "    end = time.time()\n",
        "    exectime =  (end-start)/60\n",
        "    print(len(evaluated))\n",
        "    print(len(pairs1))\n",
        "    return evaluated, pairs1, exectime\n",
        "    # score = sacrebleu.corpus_bleu(evaluated, pairs1)\n",
        "    # print(score)\n",
        "    # for i in range(len(pairs)):\n",
        "        # pair = pairs[i]\n",
        "        # print('>', pair[0])\n",
        "        # print('=', pair[1])\n",
        "        # output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        # output_sentence = ' '.join(output_words).replace('PAD', '').replace('EOS', '')\n",
        "        # print('<', output_sentence)\n",
        "        # print('')\n",
        "        # bleu = sacrebleu.corpus_bleu(pair[1], output_sentence)\n",
        "        # print(\"Score=\",bleu.score) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247,
          "referenced_widgets": [
            "2101bd95b8134007b36e552ec07655fd",
            "450e92ad344140d595ec723464737d17",
            "8df99574ee4e44e0979e106345ff1e3f",
            "975a6c2809c74ab2b0d432c012b18826",
            "395b8bbcb185443794d5bda9d08a3ea3",
            "731115d2d11048779c2f3d4ace560a92",
            "daf07a1846204eaa88e2daee9a295ab6",
            "3495e737d524446fb6e01139bc88e718"
          ]
        },
        "id": "c49TbmehyJW1",
        "outputId": "b36a9f36-1ae4-4f5d-94ad-3cbb84206767"
      },
      "source": [
        "pairs_test = read_test('testeng-fra.txt', reverse=True)\n",
        "evaluated, pairs1, exectime = evaluateFully(encoder1, attn_decoder1, test_file)\n",
        "print('execution time is', exectime, 'minutes')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counted words:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2101bd95b8134007b36e552ec07655fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=1164), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "1164\n",
            "1164\n",
            "execution time is 0.7584611694018046 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "DeUoBcp2AbCa",
        "outputId": "95b22fb1-086a-4ede-b167-60b8d09b645b"
      },
      "source": [
        "bleu = sacrebleu.corpus_bleu(evaluated, [pairs1]) \n",
        "print(bleu.score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.4354195909500966\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}